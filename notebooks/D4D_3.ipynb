{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Crime Statistics\n",
    "----------------------\n",
    "\n",
    "**Updating the MCCA data** \n",
    "\n",
    "When we finished our last session, I left you with a challenge -- see if you can get the data into shape from the Major Cities Chiefs Association for 2016-2017, and update our analysis. The original report that generated the CSV we used for 2015-2016 [is posted here](https://www.majorcitieschiefs.com/pdf/news/mcca_violent_crime_data_midyear_20162015.pdf). The updated version for 2016-2017 [is posted here](https://www.majorcitieschiefs.com/pdf/news/mcca_violent_crime_report_2017_and_2016_midyear_07312017_update.pdf). I asked that you grab the new file and see what you could do with it. I mentioned that a program like [Tabula](http://tabula.technology/) can be your friend when pulling data from tables and converting it to  more usable CSV files. How did you do? Let's review this exercise briefly, as you will have to do this kind of thing a lot. A lot. \n",
    "\n",
    "First, before we try to do anything fancy, what can you tell me about the two reports? Just look at the PDF's for a second and jot down some conclusions. First think about the data and then think about its \"structure\" -- how is it organized? How are things labled? If we want to combine the two data sets, this kind of ovservation is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put your observations here**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's be a little more systematic. I've gone to the trouble of creating CSVs from both PDFs. You can download\n",
    "[MCCA 2015-2016 here](https://github.com/cocteau/D4D/raw/master/data/mcca2016.csv) and \n",
    "[MCCA 2016-2017 here](https://github.com/cocteau/D4D/raw/master/data/mcca2017.csv). Download each and put them in the folder where you put this notebook file. We can then read them in and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 <- read.csv(\"https://github.com/cocteau/D4D/raw/master/data/mcca2016.csv\",as.is=TRUE)\n",
    "m2 <- read.csv(\"https://github.com/cocteau/D4D/raw/master/data/mcca2017.csv\",as.is=TRUE)\n",
    "\n",
    "head(m1,10)\n",
    "head(m2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(m1)\n",
    "dim(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any new observations?\n",
    "\n",
    "What we'd like to do next is \"join\" the two data sets. We are going to use the agency name as our \"key\" for this operation. That is, we want to take all the crime data for Austin, say, and create one long row that has both the 2016-2017 data as well as the 2015-2016 data. The libary dplyr comes to our rescue again with another verb. There are several kinds of \"joins\" that we can do when bringing the data from two tables together. \n",
    "\n",
    "We know, for example, that the two MCCA data sets include slightly different cities. So, how do we handle that? Do we view the current data set as our master, in some sense, and just add whatever data we can from the previous year, ignoring all the cities in 2015-2016 that don't appear in 2016-2017? Do we do it the other way? Do we only include cities that are part of both data sets? That are part of either data set? You can read about the different flavors of \"join\" by asking R for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I think we want a \"full join,\" meaning we keep all the data and just fill in with missing values (NA's) data for cities that occur in one year's report but not the other. Here's what we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcca <- full_join(m1,m2,by=\"agency\")\n",
    "head(mcca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we had columns named \"hom16\" in both data sets, R added a suffix to each to disambiguate them. I'm not wild about `.x` and `.y` so we can specify something different using the argument \"suffix\" to the `full_join()` command. What happens now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcca <- full_join(m1,m2,by=\"agency\",suffix=c(\"a\",\"b\"))\n",
    "head(mcca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `options()` command to control the way R output appears in the notebook. We have used this already when resizing our histogram plots. The `options()` call below lets us show a few more rows and columns of our data set, replacing the `...` we see in the output above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.matrix.max.cols=50, repr.matrix.max.rows=100)\n",
    "head(mcca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly for practice, let's now just look at the homicide rate. When the MCCA data came out in 2016, it was during the election cycle and candidates used them to argue about whether crime was on the rise or not. Journalistic organizations fed into this. [Here is an article from Breitbart](http://www.breitbart.com/big-government/2016/07/26/survey-violent-crime-major-cities/) and [here is one from the New York Times](https://www.nytimes.com/2016/05/14/us/murder-rates-cities-fbi.html). Remember from last time that crime statistics can be extremely variable from year to year. \n",
    "\n",
    "What does the current data say about our crime rate? Going up? Down? The `dplyr` command `select()` chooses which columns from a data frame to keep. Here we skinny things down just to homicides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homicides <- select(mcca,agency,hom17,hom16b,hom16a,hom15)\n",
    "homicides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you'll notice right away is that the counts from 2016 don't match between the data sets. Why? You also see which cities were in and out of the previous file. Why? \n",
    "\n",
    "What do you think about crime? Homicides on the rise? On the decline? Let's be more systematic. Let's filter() the data so that we keep the rows where the homicide count increased from 2015 to 2016 and from 2016 to 2017. How many cities are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(homicides,hom17-hom16b>0,hom16a-hom15>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(homicides,hom17-hom16b<=0,hom16a-hom15<=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that means there are 11 cities that saw year-on-year increases, four with decreases and the remaining 40 or so switched from up to down or down to up. So let's look back at what was written in, say, the Brietbart article.\n",
    "\n",
    ">“The biggest take away is that even though there is an increase in several violent crimes a few cities (Chicago, Phoenix, Las Vegas, Dallas, LA County, Louisville, San Antonio) account for much of the overall increase,” Darrel Stephens, the executive director of the Major Cities Chiefs Association explained in an email to Breitbart News, “Of course the tragic mass shooting in Orlando accounts for 49 of the homicides.\n",
    "<br><br>\n",
    "Of the 51 major cities’ police departments’, 29 reported increases in the number of homicides, including: Arlington PD, Atlanta PD, Aurora PD, Austin PD, Baltimore County PD, Boston PD, Chicago PD, Dallas PD, Forth Worth PD, Jacksonville Sheriff’s Dept., Las Vegas Metropolitan PD, Long Beach PD, Los Angeles County Sheriff’s Dept., Los Angeles PD, Louisville Metro PD, Nashville PD, Newark PD, Oklahoma City PD, Orlando PD, Philadelphia PD, Phoenix PD, Pittsburgh PD, Prince George’s County PD, San Antonio PD, San Diego PD, San Jose PD, Seattle PD, Tulsa PD, Washington DC (Metro PD).\n",
    "\n",
    "Chicago had an increase in the 2016-2017 report but very small one compared to the 2015-2016 number. Phoenix dropped in the recent report, as did Las Vegas, Dallas, LA County, and San Antonio. Nearly all of the cities cited by the MCCA executive director. Let's look at the percentage increase again and then compare 2016-2017 to 2015-2016. There are a lot of ways to do this, but let's keep it simple with a histogram. We'll use mutate() to create a new version of \"homicides\" that has the percentage change as columns.\n",
    "\n",
    "What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "\n",
    "homicides <- mutate(homicides,hom1716=(hom17-hom16b)/hom16b,hom1615=(hom16a-hom15)/hom15,diff=hom1716-hom1615)\n",
    "homicides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(homicides,diff< -6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goes to explain also why criminologists prefer to look at longer time periods than a single year. One event can completely skew the statistics and a blind analysis might mistake it for a trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniform Crime Reporting - The first data from the Trump Administration**\n",
    "\n",
    "We finished the last session by looking at data from the [Uniform Crime Reporting program](https://ucr.fbi.gov/) from the FBI. This aggregates crime statistics from local police and law enforcement agencies across the country. [The first report under the Trump administration](https://ucr.fbi.gov/crime-in-the-u.s/2016/crime-in-the-u.s.-2016) was released recently and was picked up by various news outlets. The Atlantic had [a long piece](https://www.theatlantic.com/politics/archive/2017/09/americas-uneven-crime-spike/541023/) examining where crime increases are happening. Let's start by verifying some of their data. \n",
    "\n",
    "For the moment, we'll focus on 2016 data and not try to compare it to 2015 as we did with the MCCA data. Toward that end, the Atlantic states\n",
    "\n",
    ">Rural, urban, and suburban communities all saw increases in violent crimes in 2016. But they were of varying degrees. Some places, like Houston and Washington, D.C., saw the number of murders either stay roughly the same or slightly decline. Other communities fared worse. Chicago ended 2016 with 762 murders, a whopping 58 percent jump over 2015’s total. Baltimore experienced its second-deadliest year on record with 358 murders, surpassing the previous record set in 2015.\n",
    "\n",
    "Let's check that out. First, we will read in the data. [I've loaded it here.](https://github.com/cocteau/D4D/raw/master/data/ucr2016.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime <- read.csv(\"https://github.com/cocteau/D4D/raw/master/data/ucr2016.csv\",as.is=TRUE)\n",
    "head(crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail(crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the functions, or verbs, added in `dplyr` is a special summary function. It helps you tell what kind of variable each column represents. This boils down to qualitative versus quantitative, a fundamental distinction as we get into analysis. Why? The function is called `glimpse()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse(crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `glimpse()` tells us that we have 9579 cities in the US reporting data to the the FBI. There are 14 columns, describing the name, state, population and crime statistics for each. Summarize the state names with a simple tabulation. That is, how many times does each state appear in the data set, or, equivalently, how many cities are included in the data set for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(group_by(crime,State),cities=n())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(crime,State==\"PENNSYLVANIA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done with the MCCA data, we can pull out columns and summarize them in various ways. Here we just look at the overall count of violent crime in the country for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(crime,total=sum(Violent_crime,na.rm=TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, the dplyr function `select()` lets us pull a number of them. Here we take City, State, Population and Violent_crime. In the `select()` function we can refer to the names of variables -- all the names are interpeted as coming from the data frame \"crime\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(select(crime,State, City, Population, Violent_crime),25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verb summarise() (Hadley is from NZ and hence the \"ise\") takes a data frame in, this time \"crime\" and returns another one, but consisting of summaries. So here we sum up all the violent crime, and in the next cell we sum crime and population across cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise(crime,Violent_total = sum(Violent_crime,na.rm=TRUE),Population_total = sum(Population,na.rm=TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets interesting when we break the data into parts. Here we group_by() the \"State\" variable and then compute the summaries. Here are the totals of violent crime and then violent crime and population by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise(group_by(crime,State),Violent_total = sum(Violent_crime,na.rm=TRUE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise(group_by(crime,State),Violent_total = sum(Violent_crime,na.rm=TRUE),Population_total = sum(Population,na.rm=TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the output of this operation in a new variable called \"state\" that we can then examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state <- summarise(group_by(crime,State),Violent_total = sum(Violent_crime,na.rm=TRUE),Population_total = sum(Population,na.rm=TRUE))\n",
    "head(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious thing to do next is introduce a new column. With `dplyr` we can use the verb `mutate()` that takes a data set like \"state\" and then adds new variables. Here we create a variable called `Violent_per100` to represent the number of violent crimes per 100,000 people in the state. (While this is just a rescaling of the \"per capita\" number, it does seem a little awkward for some states.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state <- mutate(state,Violent_per100=100000*Violent_total/Population_total)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little advanced but rather than worry about all the parentheses (passing tables as arguments to functions), we can use a \"pipe\" that takes the output of one command and uses it as input to the next. So here we take the crime data set, group it by state and the summarize the grouped data set with total population and total violent crime. Finally we take the result and add a new column to the table using mutate(). The whole thing is stored (using ->) in the data frame \"state\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime %>%\n",
    "    group_by(State) %>%\n",
    "    summarize(Violent_total = sum(Violent_crime,na.rm=TRUE),Population_total = sum(Population,na.rm=TRUE)) %>%\n",
    "    mutate(Violent_per100=100000*Violent_total/Population_total) -> state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The most dangerous cities**\n",
    "\n",
    "States are fine, but there are plenty of stories that come out each year about city-specific crime rates. The typical figure used to compare cities is again the incidents of violent crime per 100,000 people. \n",
    "\n",
    "Here we create the per 100,000 figure but for each city..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_crime <- mutate(select(crime,State,City,Population,Violent_crime),Violent_per100=100000*Violent_crime/Population)\n",
    "head(new_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "\n",
    "ggplot(new_crime,aes(x=Violent_per100))+geom_histogram(bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(new_crime,aes(x=Violent_per100))+geom_histogram(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the table using the verb `arrange().` Descending order makes sense because we want to get the most dangerous places, those with the highest incident rates at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(arrange(new_crime,desc(Violent_per100)),25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the small cities, we can `filter()`  the data, giving the same kind of condition we saw with the MCCA data. Here we look for cities with population bigger than 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_crime <- filter(new_crime,Population>100000)\n",
    "head(new_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(arrange(new_crime,desc(Violent_per100)),25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(new_crime,aes(x=Violent_per100))+geom_histogram(bins=50,color=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparisons.** For years now, certain cities have remained at the top of this list and many mayors reason that these kinds of rankings are fundamentally inaccurate because they don't compare \"apples to apples\". Essentially, city boundaries are determined on political grounds and not based on something more generalizable like population density. Some cities are essentially \"locked in place,\" having a small older inner portion of the city that contains very few low-crime suburbs. It is in this older inner core that more crime tends to happen. \n",
    "\n",
    "Have a look, for example, at [Baltimore](https://www.trulia.com/real_estate/Baltimore-Maryland/crime/) and [Detroit](https://www.trulia.com/real_estate/Detroit-Michigan/crime/). Compare these to a city like [Jacksonville, Forida.](https://www.trulia.com/real_estate/Jacksonville-Florida/crime/). In the first two cases you see that small, inner core, whereas Jacksonville city limits include relatively crime-free suburbs. It's worth looking at the historical city boundaries of Jacksonville.\n",
    "<img src=https://photos.smugmug.com/photos/564021382_VSDFJ-M.jpg>\n",
    "\n",
    "This is just one of several reasons why people try to avoid these kinds of city-to-city comparisons, even though they come out every year. [Even the FBI cautions against using rankings!](https://ucr.fbi.gov/ucr-statistics-their-proper-use).\n",
    "\n",
    "**Data publication.** As this is the first report of the Trump administration, it's worth asking if things are \"business as usual\" in the FBI or if something had changed. The FBI itself notes that [things have changed in terms of their publication of statistics this year.](https://www.fbi.gov/news/pressrel/press-releases/fbi-releases-2016-crime-statistics)\n",
    "\n",
    ">This publication is a statistical compilation of offense, arrest, and police employee data reported by law enforcement agencies voluntarily participating in the FBI’s Uniform Crime Reporting (UCR) Program. The UCR Program streamlined the 2016 edition by reducing the number of tables from 81 to 29, but still presented the major topics, such as offenses known, clearances, and persons arrested. Limited federal crime, human trafficking, and cargo theft data are also included.\n",
    "\n",
    "You can see what tables have been removed by [consulting the UCR web site.](https://ucr.fbi.gov/crime-in-the-u.s/2016/crime-in-the-u.s.-2016/tables/table-crosswalk-1) FiveThirtyEight had [a nice article on the changes to data publication.](https://fivethirtyeight.com/features/the-first-fbi-crime-report-issued-under-trump-is-missing-a-ton-of-info/) They report the following.\n",
    "\n",
    ">In response to queries from FiveThirtyEight about whether the changes to the 2016 report had been made in consultation with the Advisory Policy Board, a spokesman for the UCR responded that the program had “worked with staff from the Office of Public Affairs to review the number of times a user actually viewed the tables on the internet.” When FiveThirtyEight informed a former FBI employee of the process, he said it was abnormal.\n",
    "<br><br>\n",
    "“To me it’s shocking that they made these decisions to publish that many fewer tables and they didn’t make the decision with the APB,” James Nolan, who worked at the UCR for five years and now teaches at West Virginia University, told FiveThirtyEight.\n",
    "\n",
    "It might be worth writing to the FBI and obtaining a copy of the original data from which their tables are derived. Jeff Asher told me that the data collection remains \"as is\", it's just the tables that are being dropped. This will, he reckons, have a bigger impact on journalists who depend on the tables and are not able to work easily with the raw data (unlike, say criminologists or academics). \n",
    "\n",
    "**Other kinds of comparisons.** The UCR data has been used to make a number of other kinds of comparisons. For example, [Breitbart publishes an nearly annual story on shotgun deaths](http://www.breitbart.com/big-government/2017/10/16/fbi-over-four-times-more-people-stabbed-to-death/), observing that more people are stabbed to death with knives than are shot to death with shotguns. [Here is the raw table.](https://ucr.fbi.gov/crime-in-the-u.s/2016/crime-in-the-u.s.-2016/tables/table-12) What do you think? ([Here is what Snopes thinks](https://www.snopes.com/four-times-more-stabbed-than-rifles-any-kind/))\n",
    "\n",
    "**Filling gaps.** Before we return to more statistical work, one more comment on data that is or isn't there. For many years, the voluntary nature of the UCR meant that we had no good accounting of how many people were killed by police. Eventually, The Guardian and the Washington Post each mounted projects to try to come up with better numbers. [Here is The Guardian's piece \"The Counted\".](https://www.theguardian.com/us-news/series/counted-us-police-killings) It's worth looking at how they \"crowdsourced\" the data (partially) and how they relied on partnerships with other organizations that had been using media searches to come up with similar calculations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
